{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e6c066",
   "metadata": {},
   "source": [
    "# Three layer Fully Connected Neural Network in Pytorch\n",
    "\n",
    "Like Tensorflow, Pytorch also has two basic approaches to create the simplest deep neural network.  The **Sequential** class is the simplest and very similar to what Tensorflow calls the **Sequential API**. Each node in a layer has an activation function.  The most commonly used function is the [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) (ReLU).\n",
    "\n",
    "A fundamental difference between Tensorflow and Pytorch is that you will not see a defined input size in the definition of the Pytorch network.  Pytorch uses a [dynamic computational graph](https://medium.com/intuitionmachine/pytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1) which allows the network to adapt to the size of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02dec8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f286911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sequential_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735a8fc",
   "metadata": {},
   "source": [
    "The more elegant approach in Pytorch is to extend the **Module** class to represent the network desired.  The layers are defined in the `__init__()` method and the activations in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "672f8b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThreeLayerFccn(\n",
      "  (inputLayer): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (hiddenLayer): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (outputLayer): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as activation\n",
    "from torch import nn\n",
    "\n",
    "# Define the network using a class.\n",
    "class ThreeLayerFccn(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Call the parent constructor.\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers.\n",
    "        self.inputLayer = nn.Linear(10, 10)\n",
    "        self.hiddenLayer = nn.Linear(10,10)\n",
    "        self.outputLayer = nn.Linear(10,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # This defines a forward pass for this forward feed network.\n",
    "        x = activation.relu(self.inputLayer(x))\n",
    "        x = activation.relu(self.hiddenLayer(x))\n",
    "        return x\n",
    "    \n",
    "# Instantiate the model\n",
    "our_model = ThreeLayerFccn()\n",
    "\n",
    "print(our_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683cc93",
   "metadata": {},
   "source": [
    "The *imagined* problem addressed by this network is predicting a home sale price given 13 factors.  The network would be trained using how much it missed the real world result.  The difference between the network prediction and the actual result is termed the error. The errors are collected and summarized by a **loss function**. This network will use [Mean Square Error](https://www.youtube.com/watch?v=uD1Dfz0aqkA) as a loss function.  The value from the loss function is then used by an optimizer function to calculate how the weights on each connection should be adjusted. Here we will use [Root Mean Square Error](https://www.youtube.com/watch?v=ng629LziKrQ) as the **optimizer function**.\n",
    "\n",
    "A difference with Tensorflow, which I believe to be related to Pytorch's dynamic computational graph, is that the model is not compiled.  The loss function and optimizers then are defined outside the model definition and the resulting objects are used in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b540ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(our_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e44a0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-lab",
   "language": "python",
   "name": "pytorch-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}