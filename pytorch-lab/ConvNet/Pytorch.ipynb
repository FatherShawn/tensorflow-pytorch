{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cec9ec3",
   "metadata": {},
   "source": [
    "# Convolutional Networks for Image Classification using Pytorch\n",
    "The convolutional network architechture uses a convolutional layer paired with a MaxPool layer to roughly discern image features and filter out non-essential pixels. The results are passed to a deep learning network much like the other examples in this laboratory. Tensorflow effects padding by repeating the necessary edge pixels.  The Pytorch default is to use zeros, so here we specify a padding mode of 'replicate'.  Other available strategies are 'reflect' and 'circular'.\n",
    "\n",
    "## Padding Examples\n",
    "\n",
    "[![Full padding](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides.gif)](https://github.com/vdumoulin/conv_arithmetic)\n",
    "\n",
    "An example matrix that is 5x5:\n",
    "$$\\left[\\begin{array}{c c c c c}1&2&3&4&5\\\\\n",
    "6&7&8&9&10\\\\\n",
    "11&12&13&14&15\\\\\n",
    "16&17&18&19&20\\\\\n",
    "21&22&23&24&25\\end{array}\\right]$$\n",
    "\n",
    "For simplicty I only consider left and right padding in these examples, using a value of 2.  Replicate repeats the edge values:\n",
    "\n",
    "$$\\left[\\begin{array}{c c | c c c c c | c c}1&1&1&2&3&4&5&5&5\\\\\n",
    "6&6&6&7&8&9&10&10&10\\\\\n",
    "11&11&11&12&13&14&15&15&15\\\\\n",
    "16&16&16&17&18&19&20&20&20\\\\\n",
    "21&21&21&22&23&24&25&25&25\\end{array}\\right]$$\n",
    "\n",
    "Reflect maps the edge values as if the edge was a mirror:\n",
    "\n",
    "$$\\left[\\begin{array}{c c | c c c c c | c c}2&1&1&2&3&5&5&5&4\\\\\n",
    "7&6&6&7&8&9&10&10&9\\\\\n",
    "12&11&11&12&13&14&15&15&14\\\\\n",
    "17&16&16&17&18&19&20&20&19\\\\\n",
    "22&21&21&22&23&24&25&25&24\\end{array}\\right]$$\n",
    "\n",
    "Circular pads as if another copy of the image was tiled next to the original image.\n",
    "\n",
    "$$\\left[\\begin{array}{c c | c c c c c | c c}4&5&1&2&3&4&5&1&2\\\\\n",
    "9&10&6&7&8&9&10&6&7\\\\\n",
    "14&15&11&12&13&14&15&11&12\\\\\n",
    "19&20&16&17&18&19&20&16&17\\\\\n",
    "24&25&21&22&23&24&25&21&22\\end{array}\\right]$$\n",
    "\n",
    "## Sequential Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40a7d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), padding_mode=replicate)\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Flatten(start_dim=1, end_dim=-1)\n",
      "  (4): Linear(in_features=16384, out_features=512, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=512, out_features=26, bias=True)\n",
      "  (7): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# This model uses 128 x 128 greyscale images.\n",
    "\n",
    "sequential_model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, padding_mode='replicate'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16384, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512,26),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "print(sequential_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69ea7d",
   "metadata": {},
   "source": [
    "## Custom Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f547a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (convLayer): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), padding_mode=replicate)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (hiddenLayer): Linear(in_features=16384, out_features=512, bias=True)\n",
      "  (outputLayer): Linear(in_features=512, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as activation\n",
    "\n",
    "# This model uses 128 x 128 greyscale images.\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Call the parent constructor.\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers.\n",
    "        self.convLayer = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, padding_mode='replicate')\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.hiddenLayer = nn.Linear(16384, 512)\n",
    "        self.outputLayer = nn.Linear(512,26)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # This defines a forward pass for this forward feed network.\n",
    "        x = activation.relu(self.convLayer(x))\n",
    "        x = self.pool(x)\n",
    "        # Reshape the data to a single dimension\n",
    "        x = x.view(-1,16384)\n",
    "        x = activation.relu(self.hiddenLayer(x))\n",
    "        x = activation.softmax(self.outputlayer(x))\n",
    "        return x\n",
    "    \n",
    "# Instantiate the model\n",
    "our_model = ConvNet()\n",
    "\n",
    "print(our_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-lab",
   "language": "python",
   "name": "pytorch-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}