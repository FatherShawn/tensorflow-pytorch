{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6054718",
   "metadata": {},
   "source": [
    "# Simple Image Classifier implemented in Pytorch\n",
    "\n",
    "This simple image classifier is an adaptation of the multi-class classifier. The images are the commonly used MNIST image set of hand written digits. The 2D image matricies are flattened into 1D matricies for input.\n",
    "## Built with the Sequential Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3eb7ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, randn\n",
    "\n",
    "# Simulate a 28 x 28 pixel, grayscale \"image\"\n",
    "input = randn(1, 28, 28)\n",
    "\n",
    "# Reshape the data into a 1 dimensional tensor before use using the tensor.view() method.  A parameter of -1 is a\n",
    "# flag to let Pytorch calculate that dimension.\n",
    "\n",
    "input = input.view(1, -1)\n",
    "\n",
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(784, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512,10),\n",
    "    nn.Softmax()\n",
    ")\n",
    "print(input.shape)\n",
    "print(sequential_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b03ed",
   "metadata": {},
   "source": [
    "## Built with a Custom Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53eeb96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n",
      "MultiClassifier(\n",
      "  (inputLayer): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (hiddenLayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (outputLayer): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as activation\n",
    "from torch import nn\n",
    "\n",
    "# Reshape the data into a 1 dimensional tensor before use using the tensor.view() method.  A parameter of -1 is a\n",
    "# flag to let Pytorch calculate that dimension. This will be a preprocess step in the ingest loop just before\n",
    "# the network.\n",
    "\n",
    "input = input.view(1, -1)\n",
    "\n",
    "# Define the network using a class.\n",
    "class MultiClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Call the parent constructor.\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers.\n",
    "        self.inputLayer = nn.Linear(784, 512)\n",
    "        self.hiddenLayer = nn.Linear(512, 512)\n",
    "        self.outputLayer = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # This defines a forward pass for this forward feed network.\n",
    "        x = activation.relu(self.inputLayer(x))\n",
    "        x = activation.relu(self.hiddenLayer(x))\n",
    "        x = activation.softmax(self.outputlayer(x))\n",
    "        return x\n",
    "    \n",
    "# Instantiate the model\n",
    "our_model = MultiClassifier()\n",
    "\n",
    "print(input.shape)\n",
    "print(our_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b466a385",
   "metadata": {},
   "source": [
    "These simple networks will be prone to \"overfitting\" during training.  Overfitting is a failure to generalize.  An overfitted network is too closely aligned to the training data and much less accurate with unknown data.  A common solution to prevent overfitting is to use *dropout*, in which a given percentage of nerons do not pass on their learning.  Here are the same networks with dropout.\n",
    "## Sequential Class with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16ff395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (1): Dropout(p=0.5, inplace=False)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (4): Dropout(p=0.5, inplace=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (7): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, randn\n",
    "\n",
    "# Simulate a 28 x 28 pixel, grayscale \"image\"\n",
    "input = randn(1, 28, 28)\n",
    "\n",
    "# Reshape the data into a 1 dimensional tensor before use using the tensor.view() method.  A parameter of -1 is a\n",
    "# flag to let Pytorch calculate that dimension.\n",
    "\n",
    "input = input.view(1, -1)\n",
    "\n",
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(784, 512),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512,10),\n",
    "    nn.Softmax()\n",
    ")\n",
    "print(input.shape)\n",
    "print(sequential_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aceb632",
   "metadata": {},
   "source": [
    "## Custom Class with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5436e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784])\n",
      "MultiClassifier(\n",
      "  (inputLayer): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (hiddenLayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (outputLayer): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as activation\n",
    "from torch import nn\n",
    "\n",
    "# Reshape the data into a 1 dimensional tensor before use using the tensor.view() method.  A parameter of -1 is a\n",
    "# flag to let Pytorch calculate that dimension. This will be a preprocess step in the ingest loop just before\n",
    "# the network.\n",
    "\n",
    "input = input.view(1, -1)\n",
    "\n",
    "# Define the network using a class.\n",
    "class MultiClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Call the parent constructor.\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers.\n",
    "        self.inputLayer = nn.Linear(784, 512)\n",
    "        self.hiddenLayer = nn.Linear(512, 512)\n",
    "        self.outputLayer = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # This defines a forward pass for this forward feed network.\n",
    "        x = activation.relu(self.inputLayer(x))\n",
    "        x = self.dropout(x)\n",
    "        x = activation.relu(self.hiddenLayer(x))\n",
    "        x = self.dropout(x)\n",
    "        x = activation.softmax(self.outputlayer(x))\n",
    "        return x\n",
    "    \n",
    "# Instantiate the model\n",
    "our_model = MultiClassifier()\n",
    "\n",
    "print(input.shape)\n",
    "print(our_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38604f41",
   "metadata": {},
   "source": [
    "And Pytorch keeps the loss and optimizer functions seperate from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(our_model.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-lab",
   "language": "python",
   "name": "pytorch-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
